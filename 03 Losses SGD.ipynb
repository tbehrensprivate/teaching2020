{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Losses, Stochastic Gradient Descent\n",
    "## Dr. Tristan Behrens\n",
    "\n",
    "In the following we will lean about the essential Deep Learning building blocks. We will learn \n",
    "\n",
    "- the most common loss functions,\n",
    "- the intuition behind Stochastic Gradient Descent, and\n",
    "- strategies to overcome overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that we have TensorFlow 2 enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Their Use.\n",
    "\n",
    "These four are the most common:\n",
    "\n",
    "- Binary Crossentropy (BCE), mainly used for binary classifiers,\n",
    "- Categorical Crossentropy (CCE), mainly used for categorical classifiers,\n",
    "- Mean Squared Error (MSE), mainly used for regressions, and\n",
    "- Mean Absolute Error (MAE), mainly used for regressions, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Crossentropy.\n",
    "\n",
    "Crossentropy, in layman's terms, is the distance between probability distribtion. Binary Crossentropy is uses when we have two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "bce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Crossentropy.\n",
    "\n",
    "We use Categorical Crossentropy when we have more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0, 1, 0], [0, 0, 1]]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error.\n",
    "\n",
    "Measures the average of the quares of the errors. Mostly used in regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [1., 0.]]\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error.\n",
    "\n",
    "Measures the average of the absolutes of the errors. Mostly used in regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [1., 0.]]\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mae(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition behind Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.its.caltech.edu/~nazizanr/imgs/nonconvex3.jpg)\n",
    "\n",
    "(Image copyright Navid Azizan, Caltech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting, Best Practices.\n",
    "\n",
    "Ways to overcome underfitting:\n",
    "- Train longer,\n",
    "- bigger Neural Network architecture.\n",
    "\n",
    "Ways to overcome overfitting:\n",
    "\n",
    "- More data,\n",
    "- better data,\n",
    "- Data augmentation,\n",
    "- early stopping,\n",
    "- smaller Neural Network architecture,\n",
    "- Dropout and other regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary.\n",
    "\n",
    "In this notebook we have learned the use of loss functions when it comes to assessing the quality of Neural Networks. On top of that, we heard the intuition behind our learning algorithm Stochastic Gradient Descent. And we discussed underfitting and overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
