{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Losses, Stochastic Gradient Descent, and Natural Language Processing\n",
    "## Dr. Tristan Behrens\n",
    "\n",
    "In the following we will lean about the essential Deep Learning building blocks. We will learn \n",
    "\n",
    "- the most common loss functions,\n",
    "- the intuition behind Stochastic Gradient Descent, and\n",
    "- apply our new knowledge to Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that we have TensorFlow 2 enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Their Use.\n",
    "\n",
    "These four are the most common:\n",
    "\n",
    "- Binary Crossentropy (BCE), mainly used for binary classifiers,\n",
    "- Categorical Crossentropy (CCE), mainly used for categorical classifiers,\n",
    "- Mean Squared Error (MSE), mainly used for regressions, and\n",
    "- Mean Absolute Error (MAE), mainly used for regressions, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Crossentropy.\n",
    "\n",
    "Crossentropy, in layman's terms, is the distance between probability distribtion. Binary Crossentropy is uses when we have two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "bce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Crossentropy.\n",
    "\n",
    "We use Categorical Crossentropy when we have more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0, 1, 0], [0, 0, 1]]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error.\n",
    "\n",
    "Measures the average of the quares of the errors. Mostly used in regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [1., 0.]]\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error.\n",
    "\n",
    "Measures the average of the absolutes of the errors. Mostly used in regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [1., 0.]]\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mae(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition behind Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.its.caltech.edu/~nazizanr/imgs/nonconvex3.jpg)\n",
    "\n",
    "(Image copyright Navid Azizan, Caltech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification in Natural Language Processing - Underfitting and Overfitting.\n",
    "\n",
    "Let us solve another usecase. Sentiment Analysis is about processing a text and extracting the contained sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(imdb_train, imdb_validate, imdb_test), info = tfds.load(\n",
    "    name=\"imdb_reviews/subwords8k\", \n",
    "    split=[\"train[:80%]\", \"train[80%:]\", \"test\"],\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\", len(list(imdb_train)))\n",
    "print(\"Validate:\", len(list(imdb_validate)))\n",
    "print(\"Testing: \", len(list(imdb_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = np.array(list(imdb_train.map(lambda image, label: image)))\n",
    "print(\"Review 0 shape: \", reviews[0].shape)\n",
    "print(\"Review 1 shape: \", reviews[1].shape)\n",
    "print(\"Review 2 shape: \", reviews[2].shape)\n",
    "print(\"Review 3 shape: \", reviews[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(x) for x in reviews]\n",
    "print(\"min\", np.min(lengths))\n",
    "print(\"mean\", np.mean(lengths))\n",
    "print(\"std\", np.std(lengths))\n",
    "print(\"max\", np.max(lengths))\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(lengths, bins=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "print (\"Vocabulary size: {}\".format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A close look at the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_review, random_label = list(imdb_train.shuffle(1000).take(1))[0]\n",
    "print(\"Review:\", random_review.numpy())\n",
    "print(\"\")\n",
    "print(\"Label: \", random_label.numpy())\n",
    "print(\"\")\n",
    "random_review_decoded = encoder.decode(random_review)\n",
    "print(\"Decoded:\", random_review_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello my dear students and colleagues!\"\n",
    "\n",
    "text_encoded = encoder.encode(text)\n",
    "print(text_encoded)\n",
    "\n",
    "text_decoded = encoder.decode(text_encoded)\n",
    "print(text_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the reviews for Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = encoder.vocab_size\n",
    "\n",
    "def encode(indices, label):\n",
    "    indices = tf.dtypes.cast(indices, tf.int32)\n",
    "    review_encoded = tf.one_hot(indices=indices, depth=dimensions)\n",
    "    review_encoded = tf.reduce_max(review_encoded, 0)\n",
    "    label_encoded = label\n",
    "    return review_encoded, label_encoded\n",
    "\n",
    "imdb_train_encoded = imdb_train.map(lambda image, label: encode(image, label)).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "imdb_validate_encoded = imdb_validate.map(lambda image, label: encode(image, label)).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "imdb_test_encoded = imdb_test.map(lambda image, label: encode(image, label)).cache().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_review, random_label = list(imdb_train_encoded.shuffle(1000).take(1))[0]\n",
    "print(random_review.shape)\n",
    "print(random_review.numpy()[0:100], \"...\")\n",
    "print(np.sum(random_review))\n",
    "print(random_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(\n",
    "    16, \n",
    "    activation=\"relu\", \n",
    "    input_shape=(dimensions,)\n",
    "))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    imdb_train_encoded.batch(512),\n",
    "    epochs=20,\n",
    "    validation_data=imdb_validate_encoded.batch(512)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(imdb_test_encoded.batch(512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting, Best Practices.\n",
    "\n",
    "Ways to overcome underfitting:\n",
    "- Train longer,\n",
    "- bigger Neural Network architecture.\n",
    "\n",
    "Ways to overcome overfitting:\n",
    "\n",
    "- More data,\n",
    "- better data,\n",
    "- Data augmentation,\n",
    "- early stopping,\n",
    "- smaller Neural Network architecture,\n",
    "- Dropout and other regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary.\n",
    "\n",
    "In this notebook we have learned the use of loss functions when it comes to assessing the quality of Neural Networks. On top of that, we heard the intuition behind our learning algorithm Stochastic Gradient Descent. We hd a close look at Natural Language Processing. And we discussed underfitting and overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
