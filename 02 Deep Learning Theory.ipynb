{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Deep Learning Theory\n",
    "## Dr. Tristan Behrens\n",
    "\n",
    "In the following we will lean about the essential Deep Learning building blocks. We will learn \n",
    "\n",
    "- when ANNs are really deep, and\n",
    "- about the purpose of activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Learning vs. Reinforcement Learning.\n",
    "\n",
    "| Supervised Learning        | Unsupervised Learning            | Reinforcement Learning                 |\n",
    "| -------------------------- | -------------------------------- | -------------------------------------- |\n",
    "| mapping inputs to outputs  | looks for undiscovered patterns  | agents act in environments             |\n",
    "| classification with labels | no labels/targets                | maximizing a cumulative reward         |\n",
    "| regression with targets    | principal component analysis     | balances exploration and exploitation  |\n",
    "| optimize and generalize    | clustering                       | Markov Decision Processes              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing.\n",
    "\n",
    "Relevant steps:\n",
    "\n",
    "- Data acquisition.\n",
    "- Analysis/visualization.\n",
    "- Cleaning.\n",
    "- Balancing.\n",
    "- Normalization/standardization.\n",
    "- Encoding.\n",
    "\n",
    "Important concept: Mechanical Turk [sic!].\n",
    "\n",
    "**Question:** How much time is usually spent on data preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math behind Fully Connected Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n",
    "(Image copyright Mohamed Zahran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question:** Why do we want to have multiple layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Firstly, we define a Neural Network using TensorFlow Keras. We will keep this in mind throughout this lesson. We used a similar model for solving the MNIST problem. This model however is slightly simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, input_shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let us now move away from TensorFlow and try to emulate what the model does using pure NumPy.\n",
    "\n",
    "A fully connected or dense layer can be written like this:\n",
    "\n",
    "```\n",
    "y = dot(W, x) + b\n",
    "```\n",
    "\n",
    "What are the terms in the formula? \n",
    "\n",
    "- `x` is the input vector (tensor),\n",
    "- `W` is a weights matrix (tensor) and belongs to the trainable parameters,\n",
    "- `b` is a bias vector (tensor) and also belongs to the trainable parameters, and\n",
    "- `dot` is the dot product (matrix multiplication).\n",
    "\n",
    "Note: Of course such layers can easily be stacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we will build a multi-layer perceptron (MLP) using two fully connected layers by hand. \n",
    "\n",
    "Idea:\n",
    "\n",
    "```\n",
    "y = mlp(x)\n",
    "```\n",
    "or\n",
    "\n",
    "```\n",
    "h = layer_1(x)\n",
    "y = layer_2(h)\n",
    "```\n",
    "or\n",
    "```\n",
    "h = w1 * x + b1\n",
    "y = w2 * h + b2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "x = np.random.uniform(size=(784,))\n",
    "print(\"x: \", x.shape)\n",
    "\n",
    "# First Layer.\n",
    "w1 = np.random.randn(512, 784)\n",
    "b1 = np.random.randn(512)\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "\n",
    "# Latent result.\n",
    "h = np.dot(w1, x) + b1\n",
    "print(\"h: \", h.shape)\n",
    "\n",
    "# Second layer.\n",
    "w2 = np.random.randn(10, 512)\n",
    "b2 = np.random.randn(10)\n",
    "print(\"w2:\", w2.shape)\n",
    "print(\"b2:\", b2.shape)\n",
    "\n",
    "# Output.\n",
    "y = np.dot(w2, h) + b2\n",
    "print(\"y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Visualization always helps. How does the MLP work visually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (20, 10)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"Input x\")\n",
    "plt.imshow(x.reshape(1, -1), cmap=\"inferno\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Weights w1\")\n",
    "plt.imshow(w1, cmap=\"inferno\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Biases b1\")\n",
    "plt.imshow(b1.reshape(1, -1), cmap=\"inferno\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "plt.title(\"Hidden h = dot(w1, x) + b1\")\n",
    "plt.imshow(h.reshape(1, -1), cmap=\"inferno\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Weights w2\")\n",
    "plt.imshow(w2, cmap=\"inferno\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Biases b2\")\n",
    "plt.imshow(b2.reshape(1, -1), cmap=\"inferno\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"Output y = dot(w2, h) + b2\")\n",
    "plt.imshow(y.reshape(1, -1), cmap=\"inferno\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is this Neural Network really deep?\n",
    "\n",
    "Start with:\n",
    "```\n",
    "h = w1 * x + b1\n",
    "y = w2 * h + b2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the solution?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions.\n",
    "\n",
    "Tensorflow/Keras provide a lot of activation functions. It is also possible to implement new ones.\n",
    "\n",
    "https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    y = []\n",
    "    for number in x:\n",
    "        y.append(number)\n",
    "    return np.array(y)\n",
    "    \n",
    "x = np.linspace(-2.0, 2.0, 100).astype(\"float32\")\n",
    "y = linear(x)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Linear\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU).\n",
    "\n",
    "Easy and efficient to compute. First choice for any layer that is not an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    y = []\n",
    "    for number in x:\n",
    "        if number < 0.0:\n",
    "            y.append(0.0)\n",
    "        else:\n",
    "            y.append(number)\n",
    "    return np.array(y)\n",
    "    \n",
    "x = np.linspace(-4.0, 4.0, 100).astype(\"float32\")\n",
    "y = relu(x)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"ReLU\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid.\n",
    "\n",
    "Usually seen as output activation for either binary classifiers or multi-class binary classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1.0 / (1.0 + np.exp(-x))\n",
    "    return y\n",
    "    \n",
    "x = np.linspace(-4.0, 4.0, 100).astype(\"float32\")\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"ReLU\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh.\n",
    "\n",
    "Another choice for hidden layers of outputs between -1 and 1. Computationaly quite expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    y = np.tanh(x)\n",
    "    return y\n",
    "    \n",
    "x = np.linspace(-4.0, 4.0, 100).astype(\"float32\")\n",
    "y = tanh(x)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"tanh\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax.\n",
    "\n",
    "Usually seen as output activation in categorical classifiers. Sometimes seen as gating functions in advanced ANN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return y\n",
    "    \n",
    "x = np.random.randn(10).astype(\"float32\")\n",
    "y = softmax(x)\n",
    "\n",
    "plt.plot(x, label=\"x\")\n",
    "plt.plot(y, label=\"y = softmax(x)\")\n",
    "plt.title(\"softmax\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Activation functions.\n",
    "\n",
    "- LeakyReLU.\n",
    "- PReLU.\n",
    "- Softplus.\n",
    "- Softsign.\n",
    "- SELU.\n",
    "- ELU.\n",
    "- Exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing our example.\n",
    "\n",
    "Let us make sure that our ANN is really deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "x = np.random.uniform(size=(784,))\n",
    "print(\"x: \", x.shape)\n",
    "\n",
    "# Latent result.\n",
    "h = relu(np.dot(w1, x) + b1)\n",
    "print(\"h: \", h.shape)\n",
    "\n",
    "# Output.\n",
    "y = softmax(np.dot(w2, h) + b2)\n",
    "print(\"y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "And in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, input_shape=(784,), activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary.\n",
    "\n",
    "In this notebook we have seen the essential mathematical building blocks for ANNs. We have learned how to stack Neural Network layers. We have learned about different activation functions. And we have seen, how we can ensure that a Deep Neural Network is really deep."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
